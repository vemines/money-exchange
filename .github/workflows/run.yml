name: Schedule Update

on:
  schedule:
    - cron: '0 * * * *'      # Runs at the top of every hour
  workflow_dispatch:        # Manual trigger supported

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout code (with PAT so we can push back later)
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT }}

      # 2. Set up Node.js
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          # cache: 'npm' # Add this back if you add package.json & package-lock.json

      # 3. (Optional) Install npm deps - This step is only needed if you have package.json
      #    If you add dependencies:
      #    1. Ensure package.json and package-lock.json are committed.
      #    2. Add 'cache: 'npm'' to the setup-node step above.
      #    3. Uncomment this step:
      # - name: Install dependencies
      #   run: npm ci

      # 4. Restore Git mtimes BEFORE running scripts
      - name: Restore Git modification times (pre-script)
        continue-on-error: true
        run: |
          echo "Restoring git mtimes..."
          git ls-files -z | \
            while IFS= read -r -d $'\0' file; do
              if [ -f "$file" ]; then
                ts=$(git log -1 --format=%ct -- "$file" 2>/dev/null)
                if [ -n "$ts" ]; then # Check if timestamp is not empty
                  # Ensure date command also handles potential errors gracefully
                  formatted_time=$(date -u -d "@$ts" +'%Y%m%d%H%M.%S' 2>/dev/null)
                  if [ -n "$formatted_time" ]; then
                     touch -m -t "$formatted_time" "$file" 2>/dev/null
                  fi
                fi
              fi
            done
          echo "Pre-script mtime restoration complete."

      # 5. Fetch latest currency data
      - name: Fetch Latest Currency Data
        id: fetch_data
        run: node index.js
        env:
          APIURL: ${{ secrets.APIURL }}

      # 6. Cleanup old data (>5 years) from ./data
      - name: Cleanup Old Data (> 5 years)
        id: cleanup_data
        if: steps.fetch_data.outcome == 'success'
        run: |
          echo "Cleaning up data older than 5 years..."
          find "./data" -maxdepth 1 -type f -name '????-??-??.json' \
            -mtime +1825 -print -delete || echo "Cleanup step had warnings but continued."
          echo "Cleanup complete."

      # 7. Generate history JSON files
      - name: Generate Historical Data
        id: generate_history
        if: steps.fetch_data.outcome == 'success' && steps.cleanup_data.outcome == 'success'
        run: node generate_history.js

      # 8. Commit & push any changes back to the repo
      - name: Commit & Push Changes
        id: commit_changes
        if: steps.fetch_data.outcome == 'success' && steps.cleanup_data.outcome == 'success' && steps.generate_history.outcome == 'success'
        run: |
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          git add latest/ data/ history/
          if ! git diff --staged --quiet; then
            git commit -m "Update Currencies $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            git push https://x-access-token:${{ secrets.PAT }}@github.com/${{ github.repository }}
          else
            echo "No changes to commit."
          fi

      # 9. Sync files to Cloudflare R2 (S3-compatible) only if data gen succeeded
      - name: Sync to Cloudflare R2
        if: steps.generate_history.outcome == 'success' # Or a more comprehensive condition if needed
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ENDPOINT_URL: https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          echo "Starting sync to R2..."
          for DIR in latest data history; do
            if [ -d "$DIR" ]; then
              echo " → Syncing $DIR/"
              aws s3 sync "./$DIR/" "s3://${R2_BUCKET_NAME}/${DIR}/" \
                --endpoint-url "${R2_ENDPOINT_URL}" \
                --acl public-read \
                --delete \
                --no-progress
            else
              echo " → Directory $DIR/ not found; skipping."
            fi
          done
          echo "Cloudflare R2 sync complete."
