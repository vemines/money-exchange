name: Schedule Update

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT }} # PAT for pushing changes back to the repo

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Fetch Latest Currency Data
        id: fetch_data
        run: node index.js
        env:
          APIURL: ${{ secrets.APIURL }}
        continue-on-error: false

      - name: Cleanup Old Data (> 5 years)
        id: cleanup_data
        if: steps.fetch_data.outcome == 'success'
        run: |
          TARGET_DIR="./data"
          DAYS_OLD=1825 # Files older than approx 5 years
          echo "Cleaning up files older than $DAYS_OLD days in $TARGET_DIR..."
          find "$TARGET_DIR" -maxdepth 1 -type f -name '????-??-??.json' -mtime +$DAYS_OLD -print0 | while IFS= read -r -d $'\0' file; do
            echo "Deleting old file: $file (Last modified: $(stat -c %y "$file"))"
            rm -f "$file" || echo "Warning: Failed to delete $file"
          done
          echo "Cleanup complete. Remaining JSON files in $TARGET_DIR: $(find "$TARGET_DIR" -maxdepth 1 -type f -name '????-??-??.json' | wc -l)"

      - name: Generate Historical Data
        id: generate_history
        if: steps.fetch_data.outcome == 'success' && steps.cleanup_data.outcome == 'success'
        run: node generate_history.js

      - name: Commit and push changes
        id: commit_changes
        if: steps.fetch_data.outcome == 'success' && steps.cleanup_data.outcome == 'success' && steps.generate_history.outcome == 'success'
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add latest/ data/ history/
          if ! git diff --staged --quiet; then
            COMMIT_MSG="Update Currencies $(date -u +'%Y-%m-%d T %H-%M-%S UTC')"
            git commit -m "$COMMIT_MSG"
            git push https://x-access-token:${{ secrets.PAT }}@github.com/${{ github.repository }}
          else
            echo "No changes detected in currency data to commit."
          fi

      # AWS CLI v2 is pre-installed on ubuntu-latest runners

      - name: Upload data to Cloudflare R2
        if: steps.fetch_data.outcome == 'success' && steps.cleanup_data.outcome == 'success' && steps.generate_history.outcome == 'success'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ENDPOINT_URL: https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          echo "Verifying AWS CLI availability..."
          aws --version # Confirm aws cli is available

          # Sync 'latest' directory - files change often
          if [ -d "./latest" ]; then
            echo "Syncing './latest/' to R2 bucket '$R2_BUCKET_NAME' under 'latest/'..."
            aws s3 sync "./latest/" "s3://${R2_BUCKET_NAME}/latest/" \
              --endpoint-url "${R2_ENDPOINT_URL}" \
              --acl public-read \
              --delete
          else
            echo "Directory './latest' not found. Skipping."
          fi

          # Sync 'history' directory - files change often
          if [ -d "./history" ]; then
            echo "Syncing './history/' to R2 bucket '$R2_BUCKET_NAME' under 'history/'..."
            aws s3 sync "./history/" "s3://${R2_BUCKET_NAME}/history/" \
              --endpoint-url "${R2_ENDPOINT_URL}" \
              --acl public-read \
              --delete
          else
            echo "Directory './history' not found. Skipping."
          fi

          # Sync 'data' directory
          # Today's file might change/be new. Older files should be static locally.
          # Use --dryrun initially to see what it plans to do with older files.
          # If old files are constantly re-uploaded, it's likely due to checkout timestamps.
          if [ -d "./data" ]; then
            echo "Syncing './data/' to R2 bucket '$R2_BUCKET_NAME' under 'data/' (DRY RUN INITIALLY)..."
            aws s3 sync "./data/" "s3://${R2_BUCKET_NAME}/data/" \
              --endpoint-url "${R2_ENDPOINT_URL}" \
              --acl public-read \
              --delete \
              --dryrun # <-- REMOVE THIS AFTER TESTING AND CONFIRMING BEHAVIOR
          else
            echo "Directory './data' not found. Skipping."
          fi

          echo "R2 upload process complete."
